# Computer_vision

Computer vision is a field of study within artificial intelligence and computer science that focuses on enabling computers to gain high-level understanding from digital images or videos. By mimicking the human visual system, computer vision algorithms aim to extract meaningful information, recognize patterns, and make decisions based on visual data.

This technology involves various processes, including image acquisition, preprocessing, feature extraction, object detection, segmentation, recognition, and interpretation. It utilizes techniques like machine learning, deep learning, and neural networks to analyze and interpret visual information

1. unet_segmentation - U-Net segmentation is a convolutional neural network architecture designed for biomedical image segmentation, particularly in tasks like cell segmentation or medical image analysis. Its distinctive U-shaped structure incorporates encoder and decoder pathways to accurately delineate object boundaries within images, making it highly effective in semantic segmentation taks

2. Image_segmentation : Image segmentation is the process of partitioning a digital image into multiple segments to simplify the representation of an image or to make it more meaningful for analysis. It involves dividing an image into regions based on color, intensity, texture, or other properties, enabling object recognition and boundary delineation in computer vision tasks.

3. Canny edge detection algorithm - is a multi-stage process to detect edges in images.
   It involves:
Gaussian Blur: The input image is smoothed using a Gaussian filter to reduce noise and eliminate small variations in intensity.
Gradient Calculation: The gradients (both magnitude and direction) of the smoothed image are calculated using techniques like Sobel operators.
Non-maximum Suppression: This step thins out the edges by preserving only the local maximum pixel values in the gradient direction.
Hysteresis Thresholding: Finally, a double thresholding technique is used to determine and track potential edges by labeling pixels as strong, weak, or non-relevant based on their intensity values.
This process helps in accurately identifying edges within an image by eliminating noise and preserving important edge information.

k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
